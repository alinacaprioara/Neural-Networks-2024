{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "yiRwVjVFnCCH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "def download_mnist(is_train: bool):\n",
        "    dataset = MNIST(root='./data',\n",
        "                    transform=lambda x: np.array(x).flatten(),\n",
        "                    download=True,\n",
        "                    train=is_train)\n",
        "    mnist_data = []\n",
        "    mnist_labels = []\n",
        "    for image, label in dataset:\n",
        "        mnist_data.append(image)\n",
        "        mnist_labels.append(label)\n",
        "    return np.array(mnist_data), np.array(mnist_labels)\n",
        "\n",
        "\n",
        "train_X, train_Y = download_mnist(True)\n",
        "test_X, test_Y = download_mnist(False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = np.array(train_X)/255.0\n",
        "test_X = np.array(test_X)/255.0\n",
        "\n",
        "def convert_labels(labels):\n",
        "\n",
        "    labels = np.array(labels).astype(int)\n",
        "    classes = 10\n",
        "    matrix = np.zeros((labels.shape[0], classes))\n",
        "    matrix[np.arange(labels.shape[0]), labels] = 1\n",
        "\n",
        "    return matrix\n",
        "\n",
        "train_Y = convert_labels(train_Y)\n",
        "print(train_Y.shape[0])\n",
        "\n",
        "test_Y = convert_labels(test_Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4aEfxKn0_7B",
        "outputId": "6c7b1ca5-aa59-4699-9bb1-0f24a3fa7971"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(50)\n",
        "input_size = train_X.shape[1]  # 784\n",
        "hidden_layer = 100\n",
        "classes = 10\n",
        "dropout_rate = 0.1\n",
        "reg_lambda = 0.0001\n",
        "\n",
        "\n",
        "W1 = np.random.uniform( low=-np.sqrt(1. / (input_size + hidden_layer)), high=np.sqrt(1. / (input_size + hidden_layer)), size=(input_size, hidden_layer))\n",
        "b1 = np.zeros(hidden_layer)\n",
        "\n",
        "W2 = np.random.uniform( low=-np.sqrt(1. / (hidden_layer + classes)), high=np.sqrt(1. / (hidden_layer + classes)), size=(hidden_layer, classes))\n",
        "b2 = np.zeros(classes)"
      ],
      "metadata": {
        "id": "RwYfT4Mq1CV0"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z, clip_min=-4, clip_max=4):\n",
        "    z = np.clip(z, clip_min, clip_max)\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  #vrem pe fiecare rand\n",
        "    return exp_z / exp_z.sum(axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(y, y_pred):\n",
        "    eps = 1e-8\n",
        "    return -np.sum(y * np.log(y_pred+eps))\n",
        "\n",
        "def apply_dropout(A, rate, is_training=True):\n",
        "    if is_training:\n",
        "       mask = np.random.binomial(1, 1 - rate, size=A.shape)\n",
        "       return A * mask / (1 - rate)\n",
        "    return A\n",
        ""
      ],
      "metadata": {
        "id": "hKZvjNOx1FC9"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, W1, b1, W2, b2, learning_rate=0.01, is_training=True):\n",
        "    #forward propagation\n",
        "\n",
        "    z1 = sigmoid(X@W1 + b1)\n",
        "    z1 = apply_dropout(z1, dropout_rate, is_training=is_training)\n",
        "\n",
        "    y_pred = softmax(z1@W2 + b2)\n",
        "\n",
        "    loss = cross_entropy_loss(y, y_pred)\n",
        "    loss += reg_lambda * (np.sum(W1**2) + np.sum(W2**2))\n",
        "\n",
        "    # backward propagation\n",
        "    err_output = y_pred - y\n",
        "    dW2 = (z1.T@err_output) / X.shape[0] + reg_lambda * W2\n",
        "    db2 = np.sum(err_output, axis=0) / X.shape[0]\n",
        "\n",
        "    error_hidden = err_output@W2.T * sigmoid_derivative(z1)\n",
        "    dW1 = (X.T@error_hidden) / X.shape[0] + reg_lambda * W1\n",
        "    db1 = np.sum(error_hidden, axis = 0) / X.shape[0]\n",
        "\n",
        "    # Update weights and biases\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "\n",
        "    return W1, b1, W2, b2, loss"
      ],
      "metadata": {
        "id": "wdblq4LL1H1n"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_learning_rate(current_loss, best_loss, lr, threshold, decay_factor, min_lr, epochs_since_improvement):\n",
        "    if current_loss < best_loss:\n",
        "        best_loss = current_loss\n",
        "        epochs_since_improvement = 0\n",
        "    else:\n",
        "        epochs_since_improvement += 1\n",
        "\n",
        "\n",
        "    if epochs_since_improvement >= threshold:\n",
        "        lr = max(lr * decay_factor, min_lr)\n",
        "        epochs_since_improvement = 0\n",
        "\n",
        "    return lr, best_loss, epochs_since_improvement\n"
      ],
      "metadata": {
        "id": "hnGcTZzEQ6A6"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_scheduler(train_X, train_Y, test_X, test_Y, W1, b1, W2, b2, epochs=100, batch_size=100, initial_lr=0.01, threshold=2, decay_factor=0.2, min_lr=0.0009):\n",
        "\n",
        "    num_batches = np.ceil(train_X.shape[0] / batch_size).astype(int)\n",
        "\n",
        "\n",
        "    lr = initial_lr\n",
        "    best_loss = float('inf')\n",
        "    epochs_since_improvement = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        shuffle_indices = np.random.permutation(train_X.shape[0])\n",
        "        train_X_shuffled = train_X[shuffle_indices]\n",
        "        train_Y_shuffled = train_Y[shuffle_indices]\n",
        "\n",
        "        epoch_loss = 0\n",
        "        for i in range(num_batches):\n",
        "            start = i * batch_size\n",
        "            end = min(start + batch_size, train_X.shape[0])\n",
        "\n",
        "            X_batch = train_X_shuffled[start:end]\n",
        "            y_batch = train_Y_shuffled[start:end]\n",
        "\n",
        "\n",
        "            W1, b1, W2, b2, batch_loss = gradient_descent(X_batch, y_batch, W1, b1, W2, b2, learning_rate=lr)\n",
        "\n",
        "            epoch_loss += batch_loss\n",
        "\n",
        "        epoch_loss /= num_batches\n",
        "\n",
        "        lr, best_loss, epochs_since_improvement = update_learning_rate(\n",
        "            epoch_loss, best_loss, lr, threshold, decay_factor, min_lr, epochs_since_improvement)\n",
        "\n",
        "        train_acc = accuracy(train_X, train_Y, W1, b1, W2, b2)\n",
        "        val_acc = accuracy(test_X, test_Y, W1, b1, W2, b2)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}, Learning Rate: {lr:.6f}\")\n",
        "        print(f\"Training Accuracy: {train_acc * 100:.2f}%, Validation Accuracy: {val_acc * 100:.2f}%\\n\")\n",
        "\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "\n",
        "W1, b1, W2, b2 = train_with_scheduler(train_X, train_Y, test_X, test_Y, W1, b1, W2, b2, epochs=130, batch_size=100, initial_lr=0.01, threshold=3, decay_factor=0.2, min_lr=0.001)\n",
        "\n",
        "\n",
        "# Measure accuracy\n",
        "def accuracy(X, y, W1, b1, W2, b2):\n",
        "\n",
        "    z1 = sigmoid(X@W1 + b1)\n",
        "\n",
        "    z2 = z1@W2+ b2\n",
        "    y_pred = softmax(z2)\n",
        "\n",
        "    predicted_classes = np.argmax(y_pred, axis=1)\n",
        "    true_classes = np.argmax(y, axis=1)\n",
        "    return np.mean(predicted_classes == true_classes)\n",
        "\n",
        "\n",
        "\n",
        "test_accuracy = accuracy(test_X, test_Y, W1, b1, W2, b2)\n",
        "print(f\"Testing Data Accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1Ly0DhwRsz6",
        "outputId": "7351430a-2ee7-40ad-dd24-984e25c6a73f"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/130, Loss: 224.2051, Learning Rate: 0.010000\n",
            "Training Accuracy: 45.21%, Validation Accuracy: 46.11%\n",
            "\n",
            "Epoch 2/130, Loss: 203.2630, Learning Rate: 0.010000\n",
            "Training Accuracy: 62.52%, Validation Accuracy: 63.60%\n",
            "\n",
            "Epoch 3/130, Loss: 167.8555, Learning Rate: 0.010000\n",
            "Training Accuracy: 71.27%, Validation Accuracy: 72.04%\n",
            "\n",
            "Epoch 4/130, Loss: 132.1030, Learning Rate: 0.010000\n",
            "Training Accuracy: 76.62%, Validation Accuracy: 77.45%\n",
            "\n",
            "Epoch 5/130, Loss: 106.9828, Learning Rate: 0.010000\n",
            "Training Accuracy: 79.78%, Validation Accuracy: 80.55%\n",
            "\n",
            "Epoch 6/130, Loss: 90.8232, Learning Rate: 0.010000\n",
            "Training Accuracy: 81.79%, Validation Accuracy: 82.18%\n",
            "\n",
            "Epoch 7/130, Loss: 79.8663, Learning Rate: 0.010000\n",
            "Training Accuracy: 83.39%, Validation Accuracy: 83.92%\n",
            "\n",
            "Epoch 8/130, Loss: 72.2334, Learning Rate: 0.010000\n",
            "Training Accuracy: 84.38%, Validation Accuracy: 84.95%\n",
            "\n",
            "Epoch 9/130, Loss: 66.5252, Learning Rate: 0.010000\n",
            "Training Accuracy: 85.13%, Validation Accuracy: 85.79%\n",
            "\n",
            "Epoch 10/130, Loss: 62.2124, Learning Rate: 0.010000\n",
            "Training Accuracy: 85.84%, Validation Accuracy: 86.46%\n",
            "\n",
            "Epoch 11/130, Loss: 58.7307, Learning Rate: 0.010000\n",
            "Training Accuracy: 86.44%, Validation Accuracy: 87.03%\n",
            "\n",
            "Epoch 12/130, Loss: 56.0454, Learning Rate: 0.010000\n",
            "Training Accuracy: 86.81%, Validation Accuracy: 87.42%\n",
            "\n",
            "Epoch 13/130, Loss: 53.8982, Learning Rate: 0.010000\n",
            "Training Accuracy: 87.32%, Validation Accuracy: 87.89%\n",
            "\n",
            "Epoch 14/130, Loss: 51.7056, Learning Rate: 0.010000\n",
            "Training Accuracy: 87.53%, Validation Accuracy: 88.12%\n",
            "\n",
            "Epoch 15/130, Loss: 50.1886, Learning Rate: 0.010000\n",
            "Training Accuracy: 87.86%, Validation Accuracy: 88.43%\n",
            "\n",
            "Epoch 16/130, Loss: 48.7035, Learning Rate: 0.010000\n",
            "Training Accuracy: 88.03%, Validation Accuracy: 88.56%\n",
            "\n",
            "Epoch 17/130, Loss: 47.5577, Learning Rate: 0.010000\n",
            "Training Accuracy: 88.30%, Validation Accuracy: 88.76%\n",
            "\n",
            "Epoch 18/130, Loss: 46.5813, Learning Rate: 0.010000\n",
            "Training Accuracy: 88.49%, Validation Accuracy: 88.97%\n",
            "\n",
            "Epoch 19/130, Loss: 45.4454, Learning Rate: 0.010000\n",
            "Training Accuracy: 88.60%, Validation Accuracy: 89.07%\n",
            "\n",
            "Epoch 20/130, Loss: 44.4965, Learning Rate: 0.010000\n",
            "Training Accuracy: 88.83%, Validation Accuracy: 89.34%\n",
            "\n",
            "Epoch 21/130, Loss: 43.7761, Learning Rate: 0.010000\n",
            "Training Accuracy: 88.95%, Validation Accuracy: 89.50%\n",
            "\n",
            "Epoch 22/130, Loss: 42.9673, Learning Rate: 0.010000\n",
            "Training Accuracy: 89.03%, Validation Accuracy: 89.61%\n",
            "\n",
            "Epoch 23/130, Loss: 42.3798, Learning Rate: 0.010000\n",
            "Training Accuracy: 89.17%, Validation Accuracy: 89.64%\n",
            "\n",
            "Epoch 24/130, Loss: 41.8626, Learning Rate: 0.010000\n",
            "Training Accuracy: 89.29%, Validation Accuracy: 89.82%\n",
            "\n",
            "Epoch 25/130, Loss: 41.2693, Learning Rate: 0.010000\n",
            "Training Accuracy: 89.40%, Validation Accuracy: 89.86%\n",
            "\n",
            "Epoch 26/130, Loss: 40.7995, Learning Rate: 0.010000\n",
            "Training Accuracy: 89.47%, Validation Accuracy: 89.94%\n",
            "\n",
            "Epoch 27/130, Loss: 40.3568, Learning Rate: 0.010000\n",
            "Training Accuracy: 89.56%, Validation Accuracy: 90.02%\n",
            "\n",
            "Epoch 28/130, Loss: 39.9844, Learning Rate: 0.010000\n",
            "Training Accuracy: 89.63%, Validation Accuracy: 90.04%\n",
            "\n",
            "Epoch 29/130, Loss: 39.4762, Learning Rate: 0.010000\n",
            "Training Accuracy: 89.73%, Validation Accuracy: 90.26%\n",
            "\n",
            "Epoch 30/130, Loss: 38.9525, Learning Rate: 0.010000\n",
            "Training Accuracy: 89.83%, Validation Accuracy: 90.19%\n",
            "\n",
            "Epoch 31/130, Loss: 38.8714, Learning Rate: 0.010000\n",
            "Training Accuracy: 89.85%, Validation Accuracy: 90.27%\n",
            "\n",
            "Epoch 32/130, Loss: 38.4015, Learning Rate: 0.010000\n",
            "Training Accuracy: 89.89%, Validation Accuracy: 90.24%\n",
            "\n",
            "Epoch 33/130, Loss: 38.1362, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.01%, Validation Accuracy: 90.37%\n",
            "\n",
            "Epoch 34/130, Loss: 37.8823, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.03%, Validation Accuracy: 90.41%\n",
            "\n",
            "Epoch 35/130, Loss: 37.5833, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.12%, Validation Accuracy: 90.46%\n",
            "\n",
            "Epoch 36/130, Loss: 37.3709, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.19%, Validation Accuracy: 90.53%\n",
            "\n",
            "Epoch 37/130, Loss: 37.1757, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.24%, Validation Accuracy: 90.54%\n",
            "\n",
            "Epoch 38/130, Loss: 36.9084, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.27%, Validation Accuracy: 90.61%\n",
            "\n",
            "Epoch 39/130, Loss: 36.6972, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.29%, Validation Accuracy: 90.61%\n",
            "\n",
            "Epoch 40/130, Loss: 36.5810, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.34%, Validation Accuracy: 90.70%\n",
            "\n",
            "Epoch 41/130, Loss: 36.4250, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.39%, Validation Accuracy: 90.73%\n",
            "\n",
            "Epoch 42/130, Loss: 35.9635, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.43%, Validation Accuracy: 90.77%\n",
            "\n",
            "Epoch 43/130, Loss: 35.9898, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.48%, Validation Accuracy: 90.85%\n",
            "\n",
            "Epoch 44/130, Loss: 35.7978, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.51%, Validation Accuracy: 90.86%\n",
            "\n",
            "Epoch 45/130, Loss: 35.5409, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.54%, Validation Accuracy: 90.94%\n",
            "\n",
            "Epoch 46/130, Loss: 35.3506, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.59%, Validation Accuracy: 90.97%\n",
            "\n",
            "Epoch 47/130, Loss: 35.3284, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.61%, Validation Accuracy: 90.96%\n",
            "\n",
            "Epoch 48/130, Loss: 35.0369, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.66%, Validation Accuracy: 91.00%\n",
            "\n",
            "Epoch 49/130, Loss: 34.9807, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.69%, Validation Accuracy: 90.99%\n",
            "\n",
            "Epoch 50/130, Loss: 34.8274, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.73%, Validation Accuracy: 91.04%\n",
            "\n",
            "Epoch 51/130, Loss: 34.6346, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.73%, Validation Accuracy: 91.05%\n",
            "\n",
            "Epoch 52/130, Loss: 34.7080, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.78%, Validation Accuracy: 91.14%\n",
            "\n",
            "Epoch 53/130, Loss: 34.3045, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.80%, Validation Accuracy: 91.18%\n",
            "\n",
            "Epoch 54/130, Loss: 34.4077, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.83%, Validation Accuracy: 91.20%\n",
            "\n",
            "Epoch 55/130, Loss: 34.2390, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.83%, Validation Accuracy: 91.17%\n",
            "\n",
            "Epoch 56/130, Loss: 34.1883, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.87%, Validation Accuracy: 91.21%\n",
            "\n",
            "Epoch 57/130, Loss: 34.0694, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.86%, Validation Accuracy: 91.35%\n",
            "\n",
            "Epoch 58/130, Loss: 33.9891, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.90%, Validation Accuracy: 91.32%\n",
            "\n",
            "Epoch 59/130, Loss: 33.7651, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.92%, Validation Accuracy: 91.41%\n",
            "\n",
            "Epoch 60/130, Loss: 33.6822, Learning Rate: 0.010000\n",
            "Training Accuracy: 90.96%, Validation Accuracy: 91.40%\n",
            "\n",
            "Epoch 61/130, Loss: 33.7278, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.01%, Validation Accuracy: 91.40%\n",
            "\n",
            "Epoch 62/130, Loss: 33.6346, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.00%, Validation Accuracy: 91.43%\n",
            "\n",
            "Epoch 63/130, Loss: 33.5793, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.02%, Validation Accuracy: 91.53%\n",
            "\n",
            "Epoch 64/130, Loss: 33.4586, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.04%, Validation Accuracy: 91.55%\n",
            "\n",
            "Epoch 65/130, Loss: 33.3040, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.07%, Validation Accuracy: 91.50%\n",
            "\n",
            "Epoch 66/130, Loss: 33.2688, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.09%, Validation Accuracy: 91.50%\n",
            "\n",
            "Epoch 67/130, Loss: 33.1794, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.12%, Validation Accuracy: 91.55%\n",
            "\n",
            "Epoch 68/130, Loss: 33.1488, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.12%, Validation Accuracy: 91.56%\n",
            "\n",
            "Epoch 69/130, Loss: 33.0007, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.14%, Validation Accuracy: 91.59%\n",
            "\n",
            "Epoch 70/130, Loss: 33.1008, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.16%, Validation Accuracy: 91.57%\n",
            "\n",
            "Epoch 71/130, Loss: 32.8806, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.19%, Validation Accuracy: 91.57%\n",
            "\n",
            "Epoch 72/130, Loss: 32.9120, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.25%, Validation Accuracy: 91.63%\n",
            "\n",
            "Epoch 73/130, Loss: 32.7147, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.24%, Validation Accuracy: 91.58%\n",
            "\n",
            "Epoch 74/130, Loss: 32.8055, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.27%, Validation Accuracy: 91.62%\n",
            "\n",
            "Epoch 75/130, Loss: 32.6114, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.26%, Validation Accuracy: 91.67%\n",
            "\n",
            "Epoch 76/130, Loss: 32.5580, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.30%, Validation Accuracy: 91.69%\n",
            "\n",
            "Epoch 77/130, Loss: 32.5140, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.32%, Validation Accuracy: 91.70%\n",
            "\n",
            "Epoch 78/130, Loss: 32.4551, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.31%, Validation Accuracy: 91.71%\n",
            "\n",
            "Epoch 79/130, Loss: 32.4527, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.37%, Validation Accuracy: 91.72%\n",
            "\n",
            "Epoch 80/130, Loss: 32.4023, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.34%, Validation Accuracy: 91.72%\n",
            "\n",
            "Epoch 81/130, Loss: 32.3371, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.36%, Validation Accuracy: 91.72%\n",
            "\n",
            "Epoch 82/130, Loss: 32.3328, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.39%, Validation Accuracy: 91.80%\n",
            "\n",
            "Epoch 83/130, Loss: 32.2091, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.40%, Validation Accuracy: 91.72%\n",
            "\n",
            "Epoch 84/130, Loss: 32.1477, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.42%, Validation Accuracy: 91.77%\n",
            "\n",
            "Epoch 85/130, Loss: 32.1654, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.42%, Validation Accuracy: 91.81%\n",
            "\n",
            "Epoch 86/130, Loss: 32.0677, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.41%, Validation Accuracy: 91.79%\n",
            "\n",
            "Epoch 87/130, Loss: 32.0313, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.46%, Validation Accuracy: 91.77%\n",
            "\n",
            "Epoch 88/130, Loss: 32.0110, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.43%, Validation Accuracy: 91.80%\n",
            "\n",
            "Epoch 89/130, Loss: 31.9138, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.44%, Validation Accuracy: 91.84%\n",
            "\n",
            "Epoch 90/130, Loss: 31.8620, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.46%, Validation Accuracy: 91.81%\n",
            "\n",
            "Epoch 91/130, Loss: 31.6891, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.47%, Validation Accuracy: 91.82%\n",
            "\n",
            "Epoch 92/130, Loss: 31.8634, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.48%, Validation Accuracy: 91.86%\n",
            "\n",
            "Epoch 93/130, Loss: 31.6873, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.52%, Validation Accuracy: 91.85%\n",
            "\n",
            "Epoch 94/130, Loss: 31.8069, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.49%, Validation Accuracy: 91.89%\n",
            "\n",
            "Epoch 95/130, Loss: 31.7283, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.54%, Validation Accuracy: 91.91%\n",
            "\n",
            "Epoch 96/130, Loss: 31.6866, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.52%, Validation Accuracy: 91.92%\n",
            "\n",
            "Epoch 97/130, Loss: 31.5615, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.56%, Validation Accuracy: 91.91%\n",
            "\n",
            "Epoch 98/130, Loss: 31.5557, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.57%, Validation Accuracy: 91.89%\n",
            "\n",
            "Epoch 99/130, Loss: 31.5868, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.56%, Validation Accuracy: 91.90%\n",
            "\n",
            "Epoch 100/130, Loss: 31.5171, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.59%, Validation Accuracy: 91.94%\n",
            "\n",
            "Epoch 101/130, Loss: 31.5011, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.59%, Validation Accuracy: 91.96%\n",
            "\n",
            "Epoch 102/130, Loss: 31.4820, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.59%, Validation Accuracy: 91.93%\n",
            "\n",
            "Epoch 103/130, Loss: 31.2503, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.63%, Validation Accuracy: 91.97%\n",
            "\n",
            "Epoch 104/130, Loss: 31.2971, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.63%, Validation Accuracy: 91.99%\n",
            "\n",
            "Epoch 105/130, Loss: 31.2823, Learning Rate: 0.010000\n",
            "Training Accuracy: 91.64%, Validation Accuracy: 91.98%\n",
            "\n",
            "Epoch 106/130, Loss: 31.3880, Learning Rate: 0.002000\n",
            "Training Accuracy: 91.66%, Validation Accuracy: 92.00%\n",
            "\n",
            "Epoch 107/130, Loss: 31.2248, Learning Rate: 0.002000\n",
            "Training Accuracy: 91.67%, Validation Accuracy: 91.98%\n",
            "\n",
            "Epoch 108/130, Loss: 31.1107, Learning Rate: 0.002000\n",
            "Training Accuracy: 91.66%, Validation Accuracy: 91.98%\n",
            "\n",
            "Epoch 109/130, Loss: 31.1626, Learning Rate: 0.002000\n",
            "Training Accuracy: 91.67%, Validation Accuracy: 91.98%\n",
            "\n",
            "Epoch 110/130, Loss: 31.2895, Learning Rate: 0.002000\n",
            "Training Accuracy: 91.68%, Validation Accuracy: 91.99%\n",
            "\n",
            "Epoch 111/130, Loss: 31.1958, Learning Rate: 0.001000\n",
            "Training Accuracy: 91.68%, Validation Accuracy: 92.00%\n",
            "\n",
            "Epoch 112/130, Loss: 31.2253, Learning Rate: 0.001000\n",
            "Training Accuracy: 91.68%, Validation Accuracy: 92.00%\n",
            "\n",
            "Epoch 113/130, Loss: 31.2847, Learning Rate: 0.001000\n",
            "Training Accuracy: 91.68%, Validation Accuracy: 92.00%\n",
            "\n",
            "Epoch 114/130, Loss: 31.2873, Learning Rate: 0.001000\n",
            "Training Accuracy: 91.68%, Validation Accuracy: 91.99%\n",
            "\n",
            "Epoch 115/130, Loss: 31.2170, Learning Rate: 0.001000\n",
            "Training Accuracy: 91.68%, Validation Accuracy: 91.98%\n",
            "\n",
            "Epoch 116/130, Loss: 31.2175, Learning Rate: 0.001000\n",
            "Training Accuracy: 91.68%, Validation Accuracy: 91.98%\n",
            "\n",
            "Epoch 117/130, Loss: 31.1264, Learning Rate: 0.001000\n",
            "Training Accuracy: 91.69%, Validation Accuracy: 91.99%\n",
            "\n",
            "Epoch 118/130, Loss: 31.2344, Learning Rate: 0.001000\n",
            "Training Accuracy: 91.69%, Validation Accuracy: 91.97%\n",
            "\n",
            "Epoch 119/130, Loss: 31.0964, Learning Rate: 0.001000\n",
            "Training Accuracy: 91.69%, Validation Accuracy: 91.99%\n",
            "\n",
            "Epoch 120/130, Loss: 31.3037, Learning Rate: 0.001000\n",
            "Training Accuracy: 91.69%, Validation Accuracy: 92.00%\n",
            "\n",
            "Epoch 121/130, Loss: 31.2052, Learning Rate: 0.001000\n",
            "Training Accuracy: 91.68%, Validation Accuracy: 92.01%\n",
            "\n",
            "Epoch 122/130, Loss: 31.1004, Learning Rate: 0.001000\n",
            "Training Accuracy: 91.68%, Validation Accuracy: 92.01%\n",
            "\n",
            "Epoch 123/130, Loss: 31.1148, Learning Rate: 0.001000\n",
            "Training Accuracy: 91.69%, Validation Accuracy: 92.02%\n",
            "\n",
            "Epoch 124/130, Loss: 31.1198, Learning Rate: 0.001000\n",
            "Training Accuracy: 91.69%, Validation Accuracy: 92.04%\n",
            "\n",
            "Epoch 125/130, Loss: 31.1265, Learning Rate: 0.001000\n",
            "Training Accuracy: 91.69%, Validation Accuracy: 92.01%\n",
            "\n",
            "Epoch 126/130, Loss: 30.9918, Learning Rate: 0.001000\n",
            "Training Accuracy: 91.69%, Validation Accuracy: 92.03%\n",
            "\n",
            "Epoch 127/130, Loss: 31.2870, Learning Rate: 0.001000\n",
            "Training Accuracy: 91.69%, Validation Accuracy: 92.03%\n",
            "\n",
            "Epoch 128/130, Loss: 31.1483, Learning Rate: 0.001000\n",
            "Training Accuracy: 91.68%, Validation Accuracy: 92.03%\n",
            "\n",
            "Epoch 129/130, Loss: 31.0480, Learning Rate: 0.001000\n",
            "Training Accuracy: 91.68%, Validation Accuracy: 92.04%\n",
            "\n",
            "Epoch 130/130, Loss: 31.0615, Learning Rate: 0.001000\n",
            "Training Accuracy: 91.69%, Validation Accuracy: 92.03%\n",
            "\n",
            "Testing Data Accuracy: 92.03%\n"
          ]
        }
      ]
    }
  ]
}